{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5969452",
   "metadata": {},
   "source": [
    "## Hindsight Bias Calculation\n",
    "\n",
    "This code calculates the ex-post hindsight bias for the online bin packing instance, by computing $\\max_s \\Delta_t^\\dagger(s)$.  To adjust the parameters and recover the experiments in the paper, modify the STEP_LIMIT parameter in the binpacking environment below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9babdf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import guidedrl\n",
    "from guidedrl.bin_packing.binpacking import BinPackingLW1, BinPackingEnv, BinPackingToy, BIG_NEG_REWARD\n",
    "from guidedrl.bin_packing.optimal_bin_packing import BinPackingILP\n",
    "from guidedrl.bin_packing.common import get_exogenous_dataset, get_optimal_policy, get_ip_values\n",
    "import pulp\n",
    "import itertools\n",
    "\n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054cd606",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIN_CAPACITY = 5\n",
    "ITEM_SIZES = [2,3]\n",
    "ITEM_PROBS = [0.8, 0.2]\n",
    "STEP_LIMIT = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483f6127",
   "metadata": {},
   "source": [
    "The goal of this notebook is to understand, in a toy bin packing problem, how the 'optimal' actions are preserved via the Q^IP solution versus the Q^star solution.  The main aim is to calculate differences of the following:\n",
    "\n",
    "$\\hat{Q}(s,a') - Q^\\star(s,a') + Q^\\star(s, a^\\star) - \\hat{Q}(s,a^\\star)$\n",
    "\n",
    "where $a' = \\arg \\max_a \\hat{Q}(s,a)$ and $a^\\star = \\arg \\max_a Q^\\star(s,a)$, and $\\hat{Q}$ is the expectation of $Q^{IP}(s,a)$ over future exogenous sequences sampled independently from their respective distribution.\n",
    "\n",
    "Note that this arises from noticing that for any policy $\\mu$ we have that:\n",
    "$V^\\mu - V^\\pi = E_{d_\\pi}[Q^\\mu(s, a^\\star) - Q^\\mu(s,a')]$ and adding and subtracting terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84193c0e",
   "metadata": {},
   "source": [
    "We start by setting up the simple toy bin packing environment via the OpenAI Gym interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba0135",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BinPackingEnv()\n",
    "env.bin_capacity = BIN_CAPACITY\n",
    "env.item_sizes = ITEM_SIZES\n",
    "env.item_probs = ITEM_PROBS\n",
    "env.step_limit = STEP_LIMIT\n",
    "env._build_obs_space()\n",
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60ccfb6",
   "metadata": {},
   "source": [
    "The following code solves for the optimal $Q^*$ and $Q^\\dagger$ values in the bin packing environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c3160",
   "metadata": {},
   "outputs": [],
   "source": [
    "vStar, qStar = get_optimal_policy(env) # gets the optimal policy and IP values\n",
    "vIP, qIP = get_ip_values(env, num_iters = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91dd5a8",
   "metadata": {},
   "source": [
    "Next we compute the number of times the resulting policies are different across the state space, alongwith the maximum difference in the hindsight bias terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01022695",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_limits = np.copy(env.observation_space.high+1)\n",
    "same_count = 0\n",
    "diff_count = 0\n",
    "differences =  np.zeros(np.append(state_limits, env.step_limit))\n",
    "for h in np.arange(env.step_limit - 1, -1, -1):\n",
    "    print(f'Step: {h}')\n",
    "    for s in itertools.product(*[np.arange(x) for x in state_limits]):\n",
    "        if np.sum(list(s)[:-1]) <= h and list(s)[-1] in env.item_sizes: # in a valid state\n",
    "            ahat = np.argmax(qIP[s+(h,)])\n",
    "            astar = np.argmax(qStar[s+(h,)])\n",
    "            if ahat == astar:\n",
    "                same_count += 1\n",
    "            else:\n",
    "                diff_count += 1\n",
    "            s_hat = s+(h,)+(ahat,)\n",
    "            s_star = s+(h,)+(astar,)\n",
    "            differences[s+(h,)] = qIP[s_hat] - qStar[s_hat] + qStar[s_star] - qIP[s_star]\n",
    "print(f'Same count: {same_count} and different count: {diff_count}')\n",
    "print(f'Maximum difference: {np.max(differences)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
